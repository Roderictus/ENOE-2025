{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36eb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo Añadir deflactor del PIB trimestral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2007bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ⚠️ Saltando periodo 2020 T2 (No disponible o no oficial). ---\n",
      "--- ⚠️ Saltando periodo 2020 T3 (No disponible o no oficial). ---\n",
      "\n",
      "===========================================================\n",
      "  INICIANDO PROCESAMIENTO DE 26 TRIMESTRES\n",
      "  Rango: 2018 T1 hasta 2024 T4\n",
      "===========================================================\n",
      "\n",
      "--- ⏳ Procesando: 2018 T1 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2018_1\\ENOE_SDEMT118.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2018 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2018 T2 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2018_2\\ENOE_SDEMT218.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2018 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2018 T3 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2018_3\\ENOE_SDEMT318.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2018 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2018 T4 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2018_4\\ENOE_SDEMT418.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2018 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2019 T1 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2019_1\\ENOE_SDEMT119.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2019 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2019 T2 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2019_2\\ENOE_SDEMT219.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2019 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2019 T3 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2019_3\\ENOE_SDEMT319.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2019 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2019 T4 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2019_4\\ENOE_SDEMT419.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2019 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2020 T1 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2020_1\\ENOE_SDEMT120.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2020 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2020 T4 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2020_4\\ENOE_SDEMT420.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2020 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2021 T1 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2021_1\\ENOE_SDEMT121.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2021 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2021 T2 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2021_2\\ENOE_SDEMT221.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2021 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2021 T3 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2021_3\\ENOE_SDEMT321.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2021 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2021 T4 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2021_4\\ENOE_SDEMT421.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2021 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2022 T1 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2022_1\\ENOE_SDEMT122.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2022 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2022 T2 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2022_2\\ENOE_SDEMT222.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2022 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2022 T3 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2022_3\\ENOE_SDEMT322.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2022 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2022 T4 ---\n",
      "❌ Error Crítico: Archivo no encontrado en: data\\ENOE_dta\\ENOE_2022_4\\ENOE_SDEMT422.dta\n",
      "--- 🚫 Se agregó NA/NaN para 2022 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2023 T1 ---\n",
      "✅ Archivo cargado exitosamente. 450,263 registros.\n",
      "\n",
      "--- ⏳ Procesando: 2023 T2 ---\n",
      "✅ Archivo cargado exitosamente. 431,149 registros.\n",
      "\n",
      "--- ⏳ Procesando: 2023 T3 ---\n",
      "✅ Archivo cargado exitosamente. 429,077 registros.\n",
      "\n",
      "--- ⏳ Procesando: 2023 T4 ---\n",
      "✅ Archivo cargado exitosamente. 419,983 registros.\n",
      "\n",
      "--- ⏳ Procesando: 2024 T1 ---\n",
      "✅ Archivo cargado exitosamente. 423,866 registros.\n",
      "\n",
      "--- ⏳ Procesando: 2024 T2 ---\n",
      "✅ Archivo cargado exitosamente. 427,123 registros.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 226\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Bucle principal para procesar cada trimestre\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year, quarter \u001b[38;5;129;01min\u001b[39;00m periodos:\n\u001b[1;32m--> 226\u001b[0m     df_nacional, df_estatal \u001b[38;5;241m=\u001b[39m \u001b[43mprocesar_trimestre_enoe\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquarter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df_nacional \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m df_estatal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         resultados_nacionales\u001b[38;5;241m.\u001b[39mappend(df_nacional)\n",
      "Cell \u001b[1;32mIn[1], line 163\u001b[0m, in \u001b[0;36mprocesar_trimestre_enoe\u001b[1;34m(year, quarter, file_format)\u001b[0m\n\u001b[0;32m    161\u001b[0m df_base_est \u001b[38;5;241m=\u001b[39m df_base[df_base[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m ent_code]\n\u001b[0;32m    162\u001b[0m df_15_y_mas_est \u001b[38;5;241m=\u001b[39m df_15_y_mas[df_15_y_mas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m ent_code]\n\u001b[1;32m--> 163\u001b[0m df_pea_est \u001b[38;5;241m=\u001b[39m \u001b[43mdf_pea\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_pea\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ment_code\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    164\u001b[0m df_ocupada_est \u001b[38;5;241m=\u001b[39m df_ocupada[df_ocupada[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m ent_code]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Recolección de datos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\frame.py:4098\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4096\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   4097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 4098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   4101\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   4102\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\frame.py:4160\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   4159\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 4160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\generic.py:4172\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4161\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4164\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4165\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4170\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4172\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4173\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\generic.py:4152\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4147\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4148\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4149\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4150\u001b[0m     )\n\u001b[1;32m-> 4152\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4154\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4158\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4159\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:687\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1373\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1370\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1373\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rodri\\anaconda3\\envs\\gee\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict # Útil para recolectar datos a nivel estatal\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FUNCIONES DE UTILIDAD\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def weighted_average(df, value_col, weight_col):\n",
    "    \"\"\"Calcula el promedio ponderado de una columna usando pesos (factores de expansión).\"\"\"\n",
    "    # Asegura que las columnas de valor y peso existan y no sean NaN\n",
    "    df_filtered = df.dropna(subset=[value_col, weight_col])\n",
    "    \n",
    "    # Maneja el caso de que no haya datos o la suma de pesos sea cero\n",
    "    if df_filtered.empty or df_filtered[weight_col].sum() == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Excluir valores negativos o cero si se asume que 'ingocup' es ingreso positivo\n",
    "    # if value_col in ['ingocup', 'ing_x_hrs']:\n",
    "    #     df_filtered = df_filtered[df_filtered[value_col] > 0]\n",
    "    \n",
    "    return np.average(df_filtered[value_col], weights=df_filtered[weight_col])\n",
    "\n",
    "# Diccionario de Entidades para mapear códigos a nombres\n",
    "ENTIDADES = {\n",
    "    1: 'Aguascalientes', 2: 'Baja California', 3: 'Baja California Sur', 4: 'Campeche',\n",
    "    5: 'Coahuila', 6: 'Colima', 7: 'Chiapas', 8: 'Chihuahua', 9: 'Ciudad de México',\n",
    "    10: 'Durango', 11: 'Guanajuato', 12: 'Guerrero', 13: 'Hidalgo', 14: 'Jalisco',\n",
    "    15: 'México', 16: 'Michoacán', 17: 'Morelos', 18: 'Nayarit', 19: 'Nuevo León',\n",
    "    20: 'Oaxaca', 21: 'Puebla', 22: 'Querétaro', 23: 'Quintana Roo', 24: 'San Luis Potosí',\n",
    "    25: 'Sinaloa', 26: 'Sonora', 27: 'Tabasco', 28: 'Tamaulipas', 29: 'Tlaxcala',\n",
    "    30: 'Veracruz', 31: 'Yucatán', 32: 'Zacatecas'\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FUNCIÓN PRINCIPAL DE PROCESAMIENTO TRIMESTRAL\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def procesar_trimestre_enoe(year, quarter, file_format='dta'):\n",
    "    \"\"\"\n",
    "    Carga, limpia y calcula indicadores clave a nivel nacional y estatal \n",
    "    para un trimestre específico.\n",
    "    \n",
    "    Args:\n",
    "        year (int): El año del trimestre a analizar (e.g., 2023).\n",
    "        quarter (int): El número de trimestre (1, 2, 3, 4).\n",
    "        file_format (str): Formato del archivo ('dta' o 'csv').\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (pd.Series Nacional, pd.DataFrame Estatal) con los indicadores, \n",
    "               o (None, None) si el archivo no se encuentra o está vacío.\n",
    "    \"\"\"\n",
    "    periodo_str = f\"{year} T{quarter}\"\n",
    "    print(f\"\\n--- ⏳ Procesando: {periodo_str} ---\")\n",
    "\n",
    "    # --- 1. Construcción de la Ruta del Archivo ---\n",
    "    year_short = str(year)[-2:]\n",
    "    dir_name = f\"ENOE_{year}_{quarter}\"\n",
    "    file_name = f\"ENOE_SDEMT{quarter}{year_short}.{file_format}\"\n",
    "    file_path = os.path.join(\"data\", f\"ENOE_{file_format}\", dir_name, file_name)\n",
    "    \n",
    "    # --- 2. Carga de Datos y Manejo de Errores (Debugging) ---\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ Error Crítico: Archivo no encontrado en: {file_path}\")\n",
    "        return None, None # Retorna None para el control en el script principal\n",
    "    \n",
    "    try:\n",
    "        if file_format == 'dta':\n",
    "            df = pd.read_stata(file_path, convert_categoricals=False)\n",
    "        elif file_format == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Formato de archivo no soportado.\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"❌ Error de Carga: Archivo encontrado, pero vacío: {file_path}\")\n",
    "            return None, None\n",
    "            \n",
    "        print(f\"✅ Archivo cargado exitosamente. {len(df):,} registros.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ocurrió un error de lectura de datos: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 3. Limpieza y Preparación de Datos ---\n",
    "    # Conversión de tipos de datos esenciales\n",
    "    df['r_def'] = df['r_def'].astype(str).str.strip()\n",
    "    \n",
    "    columnas_numericas = [\n",
    "        'fac_tri', 'sex', 'eda', 'clase1', 'clase2', 'c_res',\n",
    "        'ingocup', 'ing_x_hrs', 'ent'\n",
    "    ]\n",
    "    for col in columnas_numericas:\n",
    "        # Usamos errors='coerce' para convertir valores no numéricos a NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Filtro base: Universo de residentes con entrevista completa (r_def='00', c_res=1 o 3)\n",
    "    df_base = df[(df['r_def'] == '0.0') & (df['c_res'].isin([1, 3]))].copy()\n",
    "    \n",
    "    if df_base.empty:\n",
    "        print(\"❌ Error de Filtro: No se encontraron registros válidos después del filtro base.\")\n",
    "        return None, None\n",
    "\n",
    "    # Filtro de Población en Edad de Trabajar (PET): 15 años y más\n",
    "    df_15_y_mas = df_base[df_base['eda'].between(15, 98)].copy()\n",
    "    \n",
    "    # Definición de subconjuntos\n",
    "    df_pea = df_15_y_mas[df_15_y_mas['clase1'] == 1].copy()      # PEA (clase1=1)\n",
    "    df_ocupada = df_15_y_mas[df_15_y_mas['clase2'] == 1].copy()  # Ocupada (clase2=1)\n",
    "    \n",
    "    # Asignación de nombres de estado (Necesario para ambos niveles)\n",
    "    df_base['ent_nombre'] = df_base['ent'].map(ENTIDADES)\n",
    "    df_15_y_mas['ent_nombre'] = df_15_y_mas['ent'].map(ENTIDADES)\n",
    "    df_pea['ent_nombre'] = df_pea['ent'].map(ENTIDADES)\n",
    "    df_ocupada['ent_nombre'] = df_ocupada['ent'].map(ENTIDADES)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # --- 4. CÁLCULOS A NIVEL NACIONAL ---\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    datos_nacional = {\n",
    "        # Identificadores de Tiempo\n",
    "        'year': year,\n",
    "        'quarter': quarter,\n",
    "        \n",
    "        # Población Total\n",
    "        'pob_total': df_base['fac_tri'].sum(),\n",
    "        'pob_hombres_total': df_base[df_base['sex'] == 1]['fac_tri'].sum(),\n",
    "        'pob_mujeres_total': df_base[df_base['sex'] == 2]['fac_tri'].sum(),\n",
    "        \n",
    "        # PET (15 años y más)\n",
    "        'pet_total': df_15_y_mas['fac_tri'].sum(),\n",
    "        'pet_hombres_15mas': df_15_y_mas[df_15_y_mas['sex'] == 1]['fac_tri'].sum(),\n",
    "        'pet_mujeres_15mas': df_15_y_mas[df_15_y_mas['sex'] == 2]['fac_tri'].sum(),\n",
    "\n",
    "        # PEA\n",
    "        'pea_total': df_pea['fac_tri'].sum(),\n",
    "        'pea_hombres': df_pea[df_pea['sex'] == 1]['fac_tri'].sum(),\n",
    "        'pea_mujeres': df_pea[df_pea['sex'] == 2]['fac_tri'].sum(),\n",
    "        \n",
    "        # Ingreso Promedio\n",
    "        'ing_prom_mes_total': weighted_average(df_ocupada, 'ingocup', 'fac_tri'),\n",
    "        'ing_prom_mes_hombres': weighted_average(df_ocupada[df_ocupada['sex'] == 1], 'ingocup', 'fac_tri'),\n",
    "        'ing_prom_mes_mujeres': weighted_average(df_ocupada[df_ocupada['sex'] == 2], 'ingocup', 'fac_tri'),\n",
    "        \n",
    "        'ing_prom_hora_total': weighted_average(df_ocupada, 'ing_x_hrs', 'fac_tri'),\n",
    "        'ing_prom_hora_hombres': weighted_average(df_ocupada[df_ocupada['sex'] == 1], 'ing_x_hrs', 'fac_tri'),\n",
    "        'ing_prom_hora_mujeres': weighted_average(df_ocupada[df_ocupada['sex'] == 2], 'ing_x_hrs', 'fac_tri'),\n",
    "    }\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # --- 5. CÁLCULOS A NIVEL ESTATAL ---\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    # Inicialización de un diccionario de listas para recolectar datos por estado\n",
    "    datos_estatal = defaultdict(list)\n",
    "    \n",
    "    for ent_code, ent_name in ENTIDADES.items():\n",
    "        # Filtros por Estado\n",
    "        df_base_est = df_base[df_base['ent'] == ent_code]\n",
    "        df_15_y_mas_est = df_15_y_mas[df_15_y_mas['ent'] == ent_code]\n",
    "        df_pea_est = df_pea[df_pea['ent'] == ent_code]\n",
    "        df_ocupada_est = df_ocupada[df_ocupada['ent'] == ent_code]\n",
    "        \n",
    "        # Recolección de datos\n",
    "        datos_estatal['year'].append(year)\n",
    "        datos_estatal['quarter'].append(quarter)\n",
    "        datos_estatal['ent_code'].append(ent_code)\n",
    "        datos_estatal['ent_nombre'].append(ent_name)\n",
    "        \n",
    "        # Población Total\n",
    "        datos_estatal['pob_total'].append(df_base_est['fac_tri'].sum())\n",
    "        datos_estatal['pob_hombres_total'].append(df_base_est[df_base_est['sex'] == 1]['fac_tri'].sum())\n",
    "        datos_estatal['pob_mujeres_total'].append(df_base_est[df_base_est['sex'] == 2]['fac_tri'].sum())\n",
    "\n",
    "        # PET (15 años y más)\n",
    "        datos_estatal['pet_hombres_15mas'].append(df_15_y_mas_est[df_15_y_mas_est['sex'] == 1]['fac_tri'].sum())\n",
    "        datos_estatal['pet_mujeres_15mas'].append(df_15_y_mas_est[df_15_y_mas_est['sex'] == 2]['fac_tri'].sum())\n",
    "\n",
    "        # PEA\n",
    "        datos_estatal['pea_hombres'].append(df_pea_est[df_pea_est['sex'] == 1]['fac_tri'].sum())\n",
    "        datos_estatal['pea_mujeres'].append(df_pea_est[df_pea_est['sex'] == 2]['fac_tri'].sum())\n",
    "        \n",
    "        # Ingreso Promedio\n",
    "        datos_estatal['ing_prom_mes_total'].append(weighted_average(df_ocupada_est, 'ingocup', 'fac_tri'))\n",
    "        datos_estatal['ing_prom_hora_total'].append(weighted_average(df_ocupada_est, 'ing_x_hrs', 'fac_tri'))\n",
    "\n",
    "    # Convierte el diccionario recolectado a un DataFrame\n",
    "    df_estatal_trimestre = pd.DataFrame(datos_estatal)\n",
    "    \n",
    "    return pd.Series(datos_nacional), df_estatal_trimestre\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# EJECUCIÓN DEL SCRIPT Y CONSOLIDACIÓN DE SERIES DE TIEMPO\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- RANGO DE ANÁLISIS ---\n",
    "    # Define el rango de años y trimestres a analizar\n",
    "    START_YEAR = 2018\n",
    "    END_YEAR = 2024\n",
    "    \n",
    "    # Lista para almacenar los resultados nacionales (Series de Pandas)\n",
    "    resultados_nacionales = []\n",
    "    # Lista para almacenar los resultados estatales (DataFrames)\n",
    "    resultados_estatales = []\n",
    "\n",
    "    # Genera la secuencia de trimestres\n",
    "    periodos = []\n",
    "    for y in range(START_YEAR, END_YEAR + 1):\n",
    "        for q in range(1, 5):\n",
    "            # Condición para saltarse trimestres no disponibles (ej. 2020 T2 y T3)\n",
    "            if y == 2020 and q in [2, 3]:\n",
    "                print(f\"--- ⚠️ Saltando periodo {y} T{q} (No disponible o no oficial). ---\")\n",
    "                continue\n",
    "            periodos.append((y, q))\n",
    "\n",
    "    print(f\"\\n===========================================================\")\n",
    "    print(f\"  INICIANDO PROCESAMIENTO DE {len(periodos)} TRIMESTRES\")\n",
    "    print(f\"  Rango: {START_YEAR} T1 hasta {END_YEAR} T4\")\n",
    "    print(f\"===========================================================\")\n",
    "\n",
    "    # Bucle principal para procesar cada trimestre\n",
    "    for year, quarter in periodos:\n",
    "        df_nacional, df_estatal = procesar_trimestre_enoe(year, quarter)\n",
    "        \n",
    "        if df_nacional is not None and df_estatal is not None:\n",
    "            resultados_nacionales.append(df_nacional)\n",
    "            resultados_estatales.append(df_estatal)\n",
    "        else:\n",
    "            # Manejo explícito de trimestres sin datos (se añade una fila con NA)\n",
    "            periodo_na = {'year': year, 'quarter': quarter}\n",
    "            \n",
    "            # Series Nacional con NA\n",
    "            serie_na_nacional = pd.Series(periodo_na)\n",
    "            resultados_nacionales.append(serie_na_nacional)\n",
    "            \n",
    "            # DataFrame Estatal con NA\n",
    "            df_na_estatal = pd.DataFrame(periodo_na, index=range(1, 33)) # 32 estados\n",
    "            df_na_estatal['ent_code'] = df_na_estatal.index\n",
    "            df_na_estatal['ent_nombre'] = df_na_estatal['ent_code'].map(ENTIDADES)\n",
    "            # Rellenar todas las columnas de variables con NaN\n",
    "            for col in resultados_estatales[0].columns if resultados_estatales else []:\n",
    "                if col not in df_na_estatal.columns:\n",
    "                    df_na_estatal[col] = np.nan\n",
    "            resultados_estatales.append(df_na_estatal)\n",
    "            \n",
    "            print(f\"--- 🚫 Se agregó NA/NaN para {year} T{quarter} y se prosigue. ---\")\n",
    "            \n",
    "\n",
    "    # --- 6. CONSOLIDACIÓN DE BASES DE DATOS ---\n",
    "\n",
    "    # 1. Serie de Tiempo Nacional\n",
    "    df_serie_nacional = pd.DataFrame(resultados_nacionales).reset_index(drop=True)\n",
    "    # Crea un índice de tiempo para facilitar el análisis\n",
    "    df_serie_nacional['periodo'] = df_serie_nacional['year'].astype(str) + '-T' + df_serie_nacional['quarter'].astype(str)\n",
    "    df_serie_nacional.set_index('periodo', inplace=True)\n",
    "\n",
    "    print(\"\\n===========================================================\")\n",
    "    print(\"      ✅ BASE DE SERIE DE TIEMPO NACIONAL CREADA\")\n",
    "    print(\"===========================================================\")\n",
    "    print(df_serie_nacional.head())\n",
    "    df_serie_nacional.to_csv(\"Resultados/serie_tiempo_nacional.csv\")\n",
    "\n",
    "\n",
    "    # 2. Serie de Tiempo Estatal\n",
    "    df_serie_estatal = pd.concat(resultados_estatales, ignore_index=True)\n",
    "    # Crea un índice de tiempo\n",
    "    df_serie_estatal['periodo'] = df_serie_estatal['year'].astype(str) + '-T' + df_serie_estatal['quarter'].astype(str)\n",
    "    \n",
    "    print(\"\\n===========================================================\")\n",
    "    print(\"      ✅ BASE DE SERIE DE TIEMPO ESTATAL CREADA\")\n",
    "    print(\"===========================================================\")\n",
    "    print(df_serie_estatal.head())\n",
    "    df_serie_estatal.to_csv(\"Resultados/serie_tiempo_estatal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b07800",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_stata(\"Data\\ENOE_dta\\ENOE_2005_1\\SDEMT105.dta\", convert_categoricals= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ace9c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r_def</th>\n",
       "      <th>loc</th>\n",
       "      <th>mun</th>\n",
       "      <th>est</th>\n",
       "      <th>est_d</th>\n",
       "      <th>ageb</th>\n",
       "      <th>t_loc</th>\n",
       "      <th>cd_a</th>\n",
       "      <th>ent</th>\n",
       "      <th>con</th>\n",
       "      <th>...</th>\n",
       "      <th>ma48me1sm</th>\n",
       "      <th>p14apoyos</th>\n",
       "      <th>scian</th>\n",
       "      <th>t_tra</th>\n",
       "      <th>emp_ppal</th>\n",
       "      <th>tue_ppal</th>\n",
       "      <th>trans_ppal</th>\n",
       "      <th>mh_fil2</th>\n",
       "      <th>mh_col</th>\n",
       "      <th>sec_ins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>86.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6040.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>86.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6040.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>86.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6040.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424005</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>86.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6040.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424006</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>86.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6040.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424007 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        r_def  loc  mun   est est_d  ageb t_loc  cd_a   ent     con  ...  \\\n",
       "0         0.0  NaN  3.0  24.0  0005   0.0     1   1.0   9.0   502.0  ...   \n",
       "1         0.0  NaN  3.0  24.0  0005   0.0     1   1.0   9.0   502.0  ...   \n",
       "2         0.0  NaN  3.0  24.0  0005   0.0     1   1.0   9.0   502.0  ...   \n",
       "3         0.0  NaN  7.0  33.0  0008   0.0     1   1.0   9.0   506.0  ...   \n",
       "4         0.0  NaN  7.0  33.0  0008   0.0     1   1.0   9.0   506.0  ...   \n",
       "...       ...  ...  ...   ...   ...   ...   ...   ...   ...     ...  ...   \n",
       "424002    0.0  NaN  NaN  22.0  0888   0.0     4  86.0  32.0  6040.0  ...   \n",
       "424003    0.0  NaN  NaN  22.0  0888   0.0     4  86.0  32.0  6040.0  ...   \n",
       "424004    0.0  NaN  NaN  22.0  0888   0.0     4  86.0  32.0  6040.0  ...   \n",
       "424005    0.0  NaN  NaN  22.0  0888   0.0     4  86.0  32.0  6040.0  ...   \n",
       "424006    0.0  NaN  NaN  22.0  0888   0.0     4  86.0  32.0  6040.0  ...   \n",
       "\n",
       "        ma48me1sm  p14apoyos  scian  t_tra  emp_ppal  tue_ppal  trans_ppal  \\\n",
       "0             0.0        0.0    0.0    0.0       0.0       0.0         0.0   \n",
       "1             0.0        2.0    0.0    1.0       0.0       0.0         0.0   \n",
       "2             0.0        2.0    0.0    1.0       0.0       0.0         0.0   \n",
       "3             0.0        2.0   19.0    1.0       1.0       1.0         0.0   \n",
       "4             0.0        2.0    5.0    1.0       1.0       2.0         0.0   \n",
       "...           ...        ...    ...    ...       ...       ...         ...   \n",
       "424002        1.0        2.0    1.0    1.0       1.0       2.0         0.0   \n",
       "424003        0.0        1.0    0.0    1.0       0.0       0.0         0.0   \n",
       "424004        0.0        0.0    0.0    0.0       0.0       0.0         0.0   \n",
       "424005        0.0        1.0    0.0    1.0       0.0       0.0         0.0   \n",
       "424006        0.0        1.0    0.0    1.0       0.0       0.0         0.0   \n",
       "\n",
       "        mh_fil2  mh_col  sec_ins  \n",
       "0           0.0     0.0      0.0  \n",
       "1           0.0     0.0      0.0  \n",
       "2           0.0     0.0      0.0  \n",
       "3           1.0     1.0      8.0  \n",
       "4           3.0     1.0      4.0  \n",
       "...         ...     ...      ...  \n",
       "424002      4.0     1.0      3.0  \n",
       "424003      0.0     0.0      0.0  \n",
       "424004      0.0     0.0      0.0  \n",
       "424005      0.0     0.0      0.0  \n",
       "424006      0.0     0.0      0.0  \n",
       "\n",
       "[424007 rows x 104 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288997d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['r_def', 'loc', 'mun', 'est', 'est_d', 'ageb', 't_loc', 'cd_a', 'ent',\n",
       "       'con',\n",
       "       ...\n",
       "       'ma48me1sm', 'p14apoyos', 'scian', 't_tra', 'emp_ppal', 'tue_ppal',\n",
       "       'trans_ppal', 'mh_fil2', 'mh_col', 'sec_ins'],\n",
       "      dtype='object', length=104)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Definición del Rango de la Serie de Tiempo ---\n",
      "--- ⚠️ Saltando periodo 2020 T2 (No disponible o no oficial). ---\n",
      "--- ⚠️ Saltando periodo 2020 T3 (No disponible o no oficial). ---\n",
      "\n",
      "===========================================================\n",
      "  INICIANDO PROCESAMIENTO DE 80 TRIMESTRES\n",
      "===========================================================\n",
      "\n",
      "--- ⏳ Procesando: 2005 T1 ---\n",
      "✅ Archivo cargado exitosamente. 424,007 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2005 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2005 T2 ---\n",
      "✅ Archivo cargado exitosamente. 428,727 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2005 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2005 T3 ---\n",
      "✅ Archivo cargado exitosamente. 421,751 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2005 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2005 T4 ---\n",
      "✅ Archivo cargado exitosamente. 423,757 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2005 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2006 T1 ---\n",
      "✅ Archivo cargado exitosamente. 426,160 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2006 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2006 T2 ---\n",
      "✅ Archivo cargado exitosamente. 424,579 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2006 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2006 T3 ---\n",
      "✅ Archivo cargado exitosamente. 423,305 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2006 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2006 T4 ---\n",
      "✅ Archivo cargado exitosamente. 421,581 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2006 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2007 T1 ---\n",
      "✅ Archivo cargado exitosamente. 423,910 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2007 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2007 T2 ---\n",
      "✅ Archivo cargado exitosamente. 422,591 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2007 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2007 T3 ---\n",
      "✅ Archivo cargado exitosamente. 418,327 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2007 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2007 T4 ---\n",
      "✅ Archivo cargado exitosamente. 409,422 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2007 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2008 T1 ---\n",
      "✅ Archivo cargado exitosamente. 416,538 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2008 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2008 T2 ---\n",
      "✅ Archivo cargado exitosamente. 415,610 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2008 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2008 T3 ---\n",
      "✅ Archivo cargado exitosamente. 410,219 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2008 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2008 T4 ---\n",
      "✅ Archivo cargado exitosamente. 407,232 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2008 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2009 T1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\AppData\\Local\\Temp\\ipykernel_1416\\43258366.py:140: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  df = pd.read_stata(file_path, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo cargado exitosamente. 407,725 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2009 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2009 T2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\AppData\\Local\\Temp\\ipykernel_1416\\43258366.py:140: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  df = pd.read_stata(file_path, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo cargado exitosamente. 405,529 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2009 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2009 T3 ---\n",
      "✅ Archivo cargado exitosamente. 402,919 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2009 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2009 T4 ---\n",
      "✅ Archivo cargado exitosamente. 403,862 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2009 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2010 T1 ---\n",
      "✅ Archivo cargado exitosamente. 406,797 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2010 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2010 T2 ---\n",
      "✅ Archivo cargado exitosamente. 408,164 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2010 T2 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2010 T3 ---\n",
      "✅ Archivo cargado exitosamente. 405,533 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2010 T3 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2010 T4 ---\n",
      "✅ Archivo cargado exitosamente. 401,524 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2010 T4 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2011 T1 ---\n",
      "✅ Archivo cargado exitosamente. 402,117 registros. Ponderador: FAC\n",
      "❌ Error Crítico: Columna de ponderador 'FAC' no encontrada.\n",
      "--- 🚫 Se agregó NA/NaN para 2011 T1 y se prosigue. ---\n",
      "\n",
      "--- ⏳ Procesando: 2011 T2 ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FUNCIONES DE UTILIDAD\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def weighted_average(df, value_col, weight_col):\n",
    "    \"\"\"Calcula el promedio ponderado de una columna usando pesos (factores de expansión).\"\"\"\n",
    "    df_filtered = df.dropna(subset=[value_col, weight_col])\n",
    "    \n",
    "    # Excluir valores de ingreso no válidos (generalmente negativos o no especificados, si se aplica)\n",
    "    if value_col in ['ingocup', 'ing_x_hrs']:\n",
    "        df_filtered = df_filtered[df_filtered[value_col] > 0].copy()\n",
    "    \n",
    "    if df_filtered.empty or df_filtered[weight_col].sum() == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return np.average(df_filtered[value_col], weights=df_filtered[weight_col])\n",
    "\n",
    "# Diccionario de Entidades para mapear códigos a nombres\n",
    "ENTIDADES = {\n",
    "    1: 'Aguascalientes', 2: 'Baja California', 3: 'Baja California Sur', 4: 'Campeche',\n",
    "    5: 'Coahuila', 6: 'Colima', 7: 'Chiapas', 8: 'Chihuahua', 9: 'Ciudad de México',\n",
    "    10: 'Durango', 11: 'Guanajuato', 12: 'Guerrero', 13: 'Hidalgo', 14: 'Jalisco',\n",
    "    15: 'México', 16: 'Michoacán', 17: 'Morelos', 18: 'Nayarit', 19: 'Nuevo León',\n",
    "    20: 'Oaxaca', 21: 'Puebla', 22: 'Querétaro', 23: 'Quintana Roo', 24: 'San Luis Potosí',\n",
    "    25: 'Sinaloa', 26: 'Sonora', 27: 'Tabasco', 28: 'Tamaulipas', 29: 'Tlaxcala',\n",
    "    30: 'Veracruz', 31: 'Yucatán', 32: 'Zacatecas'\n",
    "}\n",
    "\n",
    "def obtener_nombre_archivo(year, quarter, file_format='dta'):\n",
    "    \"\"\"Determina el nombre del archivo SDEMT según el periodo.\"\"\"\n",
    "    year_short = str(year)[-2:]\n",
    "    \n",
    "    # Periodo 1: 2005 T1 a 2018 T4 (Mayúsculas)\n",
    "    if year <= 2018:\n",
    "        base_name = f\"SDEMT{quarter}{year_short}\".upper()\n",
    "    \n",
    "    # Periodo 2: 2019 T1 a 2019 T4 (Minúsculas)\n",
    "    elif year == 2019:\n",
    "        base_name = f\"sdemt{quarter}{year_short}\".lower()\n",
    "    \n",
    "    # Periodo 3: 2020 T3 a 2022 T4 (Prefijo ENOEN_)\n",
    "    elif 2020 <= year <= 2022:\n",
    "        # 2020 T1 y T2 no tienen datos o son no oficiales (se manejan como \"saltados\" en el script principal)\n",
    "        base_name = f\"ENOEN_SDEMT{quarter}{year_short}\".upper()\n",
    "    \n",
    "    # Periodo 4: 2023 T1 en adelante (Vuelve a Mayúsculas/Patrón consistente con el documento)\n",
    "    else: # year >= 2023\n",
    "        base_name = f\"SDEMT{quarter}{year_short}\".upper()\n",
    "        \n",
    "    dir_name = f\"ENOE_{year}_{quarter}\"\n",
    "    file_name = f\"{base_name}.{file_format}\"\n",
    "    # Asume que los archivos están en data/dta/ENOE_YYYY_Q/SDEMT...\n",
    "    #file_path = os.path.join(\"Data/\", file_format, dir_name, file_name) \n",
    "    file_path = os.path.join(\"Data/ENOE_dta\", dir_name, file_name) \n",
    "    return file_path\n",
    "\n",
    "def pedir_rango_trimestral():\n",
    "    \"\"\"Pide al usuario el rango de años y trimestres para generar la serie de tiempo.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"\\n--- Definición del Rango de la Serie de Tiempo ---\")\n",
    "            start_year = int(input(\"Ingrese el AÑO de inicio (e.g., 2018): \"))\n",
    "            start_quarter = int(input(\"Ingrese el TRIMESTRE de inicio (1 a 4): \"))\n",
    "            end_year = int(input(\"Ingrese el AÑO final (e.g., 2024): \"))\n",
    "            end_quarter = int(input(\"Ingrese el TRIMESTRE final (1 a 4): \"))\n",
    "            \n",
    "            if not (1 <= start_quarter <= 4 and 1 <= end_quarter <= 4):\n",
    "                raise ValueError(\"El trimestre debe ser un número entre 1 y 4.\")\n",
    "            \n",
    "            start_date = datetime(start_year, start_quarter * 3 - 2, 1)\n",
    "            end_date = datetime(end_year, end_quarter * 3 - 2, 1)\n",
    "\n",
    "            if start_date > end_date:\n",
    "                raise ValueError(\"El periodo de inicio debe ser anterior o igual al periodo final.\")\n",
    "                \n",
    "            break\n",
    "        except ValueError as e:\n",
    "            print(f\"Entrada inválida: {e}. Por favor, intente de nuevo.\")\n",
    "            \n",
    "    # Generar la secuencia de trimestres\n",
    "    periodos = []\n",
    "    current_year = start_year\n",
    "    current_quarter = start_quarter\n",
    "    \n",
    "    while current_year < end_year or (current_year == end_year and current_quarter <= end_quarter):\n",
    "        \n",
    "        # Manejo de trimestres faltantes (2020 T2 y T3 no oficiales/disponibles)\n",
    "        if current_year == 2020 and current_quarter in [2, 3]:\n",
    "            print(f\"--- ⚠️ Saltando periodo {current_year} T{current_quarter} (No disponible o no oficial). ---\")\n",
    "            pass # No se añade el periodo a la lista para no intentar cargarlo.\n",
    "            \n",
    "        else:\n",
    "            periodos.append((current_year, current_quarter))\n",
    "            \n",
    "        # Pasar al siguiente trimestre\n",
    "        if current_quarter == 4:\n",
    "            current_quarter = 1\n",
    "            current_year += 1\n",
    "        else:\n",
    "            current_quarter += 1\n",
    "            \n",
    "    return periodos\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FUNCIÓN PRINCIPAL DE PROCESAMIENTO TRIMESTRAL\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def procesar_trimestre_enoe(year, quarter, file_format='dta'):\n",
    "    \"\"\"\n",
    "    Carga, limpia y calcula indicadores clave a nivel nacional y estatal \n",
    "    para un trimestre específico.\n",
    "    \"\"\"\n",
    "    periodo_str = f\"{year} T{quarter}\"\n",
    "    print(f\"\\n--- ⏳ Procesando: {periodo_str} ---\")\n",
    "\n",
    "    # --- 1. Obtener Ruta y Ponderador ---\n",
    "    file_path = obtener_nombre_archivo(year, quarter, file_format)\n",
    "    \n",
    "    # Determinar el campo ponderador correcto según el periodo \n",
    "    if year < 2020 or (year == 2020 and quarter < 3):\n",
    "        PONDERATOR = 'FAC'\n",
    "    else:\n",
    "        PONDERATOR = 'FAC_TRI'\n",
    "    \n",
    "    # --- 2. Carga de Datos y Manejo de Errores (Debugging) ---\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ Error Crítico: Archivo no encontrado en: {file_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        if file_format == 'dta':\n",
    "            # Se usa `encoding='latin-1'` si se encuentran problemas con codificación de texto\n",
    "            df = pd.read_stata(file_path, convert_categoricals=False) \n",
    "        elif file_format == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Formato de archivo no soportado.\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"❌ Error de Carga: Archivo encontrado, pero vacío: {file_path}\")\n",
    "            return None, None\n",
    "            \n",
    "        print(f\"✅ Archivo cargado exitosamente. {len(df):,} registros. Ponderador: {PONDERATOR}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ocurrió un error de lectura de datos en {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 3. Limpieza y Preparación de Datos ---\n",
    "    \n",
    "    # Conversión de tipos de datos esenciales y estandarización de nombres\n",
    "    columnas_requeridas = [\n",
    "        PONDERATOR, 'sex', 'eda', 'clase1', 'clase2', 'c_res', 'r_def', 'ent',\n",
    "        'ingocup', 'ing_x_hrs', 'pos_ocu', 'emp_ppal', 'sub_o' # Indicadores estratégicos\n",
    "    ]\n",
    "    \n",
    "    for col in columnas_requeridas:\n",
    "        if col not in df.columns:\n",
    "            # Añadir columna con NaN/0 si falta, para evitar errores en cálculos posteriores (excepto ponderador)\n",
    "            if col == PONDERATOR:\n",
    "                 print(f\"❌ Error Crítico: Columna de ponderador '{PONDERATOR}' no encontrada.\")\n",
    "                 return None, None\n",
    "            df[col] = np.nan if col not in ['r_def', 'c_res'] else 0\n",
    "            print(f\"⚠️ Columna '{col}' no encontrada. Se añadió con NaN/0 para proseguir.\")\n",
    "\n",
    "    # Conversión de tipos\n",
    "    df['r_def'] = df['r_def'].astype(str).str.strip()\n",
    "    for col in ['sex', 'eda', 'clase1', 'clase2', 'c_res', 'ent', 'pos_ocu', 'emp_ppal', 'sub_o']:\n",
    "         # Convertir a numérico, forzando errores a NaN, luego a entero (si es posible)\n",
    "         df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "    for col in ['ingocup', 'ing_x_hrs', PONDERATOR]:\n",
    "         df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "\n",
    "    # CRITERIO GENERAL DE FILTRADO (POBLACIÓN DE 15 AÑOS Y MÁS)\n",
    "    # R_DEF='00' y (C_RES=1 o 3) y (EDA>=15 y EDA<=98) [cite: 148]\n",
    "    \n",
    "    # 1. Población total residente\n",
    "    df_base = df[(df['r_def'] == '00') & (df['c_res'].isin([1, 3]))].copy()\n",
    "\n",
    "    # 2. Población en Edad de Trabajar (PET) 15 años y más\n",
    "    df_15_y_mas = df_base[df_base['eda'].between(15, 98)].copy()\n",
    "    \n",
    "    if df_15_y_mas.empty:\n",
    "        print(\"❌ Error de Filtro: No se encontraron registros válidos después del filtro PET.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Asignación de nombres de estado\n",
    "    df_base['ent_nombre'] = df_base['ent'].map(ENTIDADES)\n",
    "    df_15_y_mas['ent_nombre'] = df_15_y_mas['ent'].map(ENTIDADES)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # --- 4. CÁLCULOS A NIVEL NACIONAL ---\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    # Subconjuntos basados en campos precodificados y el criterio general [cite: 147]\n",
    "    df_pea = df_15_y_mas[df_15_y_mas['clase1'] == 1].copy()      \n",
    "    df_pnea = df_15_y_mas[df_15_y_mas['clase1'] == 2].copy()\n",
    "    df_ocupada = df_15_y_mas[df_15_y_mas['clase2'] == 1].copy() \n",
    "\n",
    "    datos_nacional = {\n",
    "        # Identificadores de Tiempo\n",
    "        'year': year,\n",
    "        'quarter': quarter,\n",
    "        \n",
    "        # 1. Población\n",
    "        'pob_total': df_base[PONDERATOR].sum(),\n",
    "        'pob_15_y_mas': df_15_y_mas[PONDERATOR].sum(), # [cite: 162]\n",
    "        'pob_hombres_total': df_base[df_base['sex'] == 1][PONDERATOR].sum(),\n",
    "        'pob_mujeres_total': df_base[df_base['sex'] == 2][PONDERATOR].sum(),\n",
    "        \n",
    "        # 2. PEA y PNEA\n",
    "        'pea_total': df_pea[PONDERATOR].sum(),\n",
    "        'pea_hombres': df_pea[df_pea['sex'] == 1][PONDERATOR].sum(),\n",
    "        'pea_mujeres': df_pea[df_pea['sex'] == 2][PONDERATOR].sum(),\n",
    "        'pnea_total': df_pnea[PONDERATOR].sum(), # [cite: 163]\n",
    "        \n",
    "        # Indicadores Estratégicos (CLASE2 y CLASE1) [cite: 162, 163]\n",
    "        'ocupada_total': df_ocupada[PONDERATOR].sum(),\n",
    "        'desocupada_total': df_15_y_mas[df_15_y_mas['clase2'] == 2][PONDERATOR].sum(),\n",
    "        'pnea_disponible': df_15_y_mas[df_15_y_mas['clase2'] == 3][PONDERATOR].sum(),\n",
    "        'pnea_no_disponible': df_15_y_mas[df_15_y_mas['clase2'] == 4][PONDERATOR].sum(),\n",
    "        \n",
    "        # Indicadores Estratégicos (POSICIÓN EN LA OCUPACIÓN - Ocupados) [cite: 163]\n",
    "        'subordinados_remunerados': df_ocupada[df_ocupada['pos_ocu'] == 1][PONDERATOR].sum(),\n",
    "        'empleadores': df_ocupada[df_ocupada['pos_ocu'] == 2][PONDERATOR].sum(),\n",
    "        'cuenta_propia': df_ocupada[df_ocupada['pos_ocu'] == 3][PONDERATOR].sum(),\n",
    "        'trabajadores_no_remunerados': df_ocupada[df_ocupada['pos_ocu'] == 4][PONDERATOR].sum(),\n",
    "        \n",
    "        # Indicadores Estratégicos (CONDICIÓN DE INFORMALIDAD - Ocupados) [cite: 169]\n",
    "        'ocupacion_formal': df_ocupada[df_ocupada['emp_ppal'] == 2][PONDERATOR].sum(),\n",
    "        'ocupacion_informal': df_ocupada[df_ocupada['emp_ppal'] == 1][PONDERATOR].sum(),\n",
    "        \n",
    "        # Indicador Estratégico (SUBOCUPACIÓN - Ocupados)\n",
    "        'subocupacion': df_ocupada[df_ocupada['sub_o'] == 1][PONDERATOR].sum(),\n",
    "        \n",
    "        # 3. Ingreso Promedio\n",
    "        'ing_prom_mes_total': weighted_average(df_ocupada, 'ingocup', PONDERATOR),\n",
    "        'ing_prom_hora_total': weighted_average(df_ocupada, 'ing_x_hrs', PONDERATOR),\n",
    "    }\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # --- 5. CÁLCULOS A NIVEL ESTATAL ---\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    datos_estatal = defaultdict(list)\n",
    "    \n",
    "    for ent_code, ent_name in ENTIDADES.items():\n",
    "        # Filtros base por Estado (Criterio General)\n",
    "        df_base_est = df_base[df_base['ent'] == ent_code].copy()\n",
    "        df_15_y_mas_est = df_15_y_mas[df_15_y_mas['ent'] == ent_code].copy()\n",
    "        \n",
    "        # Subconjuntos Estatales (basados en precodificados y el filtro base estatal)\n",
    "        df_pea_est = df_15_y_mas_est[df_15_y_mas_est['clase1'] == 1].copy()\n",
    "        df_pnea_est = df_15_y_mas_est[df_15_y_mas_est['clase1'] == 2].copy()\n",
    "        df_ocupada_est = df_15_y_mas_est[df_15_y_mas_est['clase2'] == 1].copy()\n",
    "        \n",
    "        # Recolección de datos\n",
    "        datos_estatal['year'].append(year)\n",
    "        datos_estatal['quarter'].append(quarter)\n",
    "        datos_estatal['ent_code'].append(ent_code)\n",
    "        datos_estatal['ent_nombre'].append(ent_name)\n",
    "        \n",
    "        # Población\n",
    "        datos_estatal['pob_total'].append(df_base_est[PONDERATOR].sum())\n",
    "        datos_estatal['pob_15_y_mas'].append(df_15_y_mas_est[PONDERATOR].sum())\n",
    "        datos_estatal['pob_hombres_total'].append(df_base_est[df_base_est['sex'] == 1][PONDERATOR].sum())\n",
    "        datos_estatal['pob_mujeres_total'].append(df_base_est[df_base_est['sex'] == 2][PONDERATOR].sum())\n",
    "        \n",
    "        # PEA y PNEA\n",
    "        datos_estatal['pea_total'].append(df_pea_est[PONDERATOR].sum())\n",
    "        datos_estatal['pnea_total'].append(df_pnea_est[PONDERATOR].sum())\n",
    "        \n",
    "        # Indicadores Estratégicos (CLASE2 y CLASE1)\n",
    "        datos_estatal['ocupada_total'].append(df_ocupada_est[PONDERATOR].sum())\n",
    "        datos_estatal['desocupada_total'].append(df_15_y_mas_est[df_15_y_mas_est['clase2'] == 2][PONDERATOR].sum())\n",
    "        datos_estatal['pnea_disponible'].append(df_15_y_mas_est[df_15_y_mas_est['clase2'] == 3][PONDERATOR].sum())\n",
    "        datos_estatal['pnea_no_disponible'].append(df_15_y_mas_est[df_15_y_mas_est['clase2'] == 4][PONDERATOR].sum())\n",
    "        \n",
    "        # Indicadores Estratégicos (POSICIÓN EN LA OCUPACIÓN - Ocupados)\n",
    "        datos_estatal['subordinados_remunerados'].append(df_ocupada_est[df_ocupada_est['pos_ocu'] == 1][PONDERATOR].sum())\n",
    "        datos_estatal['empleadores'].append(df_ocupada_est[df_ocupada_est['pos_ocu'] == 2][PONDERATOR].sum())\n",
    "        datos_estatal['cuenta_propia'].append(df_ocupada_est[df_ocupada_est['pos_ocu'] == 3][PONDERATOR].sum())\n",
    "        datos_estatal['trabajadores_no_remunerados'].append(df_ocupada_est[df_ocupada_est['pos_ocu'] == 4][PONDERATOR].sum())\n",
    "\n",
    "        # Indicadores Estratégicos (CONDICIÓN DE INFORMALIDAD - Ocupados)\n",
    "        datos_estatal['ocupacion_formal'].append(df_ocupada_est[df_ocupada_est['emp_ppal'] == 2][PONDERATOR].sum())\n",
    "        datos_estatal['ocupacion_informal'].append(df_ocupada_est[df_ocupada_est['emp_ppal'] == 1][PONDERATOR].sum())\n",
    "        \n",
    "        # Indicador Estratégico (SUBOCUPACIÓN - Ocupados)\n",
    "        datos_estatal['subocupacion'].append(df_ocupada_est[df_ocupada_est['sub_o'] == 1][PONDERATOR].sum())\n",
    "\n",
    "        # Ingreso Promedio\n",
    "        datos_estatal['ing_prom_mes_total'].append(weighted_average(df_ocupada_est, 'ingocup', PONDERATOR))\n",
    "        datos_estatal['ing_prom_hora_total'].append(weighted_average(df_ocupada_est, 'ing_x_hrs', PONDERATOR))\n",
    "\n",
    "    df_estatal_trimestre = pd.DataFrame(datos_estatal)\n",
    "    \n",
    "    return pd.Series(datos_nacional), df_estatal_trimestre\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# EJECUCIÓN DEL SCRIPT Y CONSOLIDACIÓN DE SERIES DE TIEMPO\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    periodos = pedir_rango_trimestral()\n",
    "    \n",
    "    # Inicialización para la consolidación\n",
    "    resultados_nacionales = []\n",
    "    resultados_estatales = []\n",
    "\n",
    "    print(f\"\\n===========================================================\")\n",
    "    print(f\"  INICIANDO PROCESAMIENTO DE {len(periodos)} TRIMESTRES\")\n",
    "    print(f\"===========================================================\")\n",
    "\n",
    "    # Bucle principal para procesar cada trimestre\n",
    "    for year, quarter in periodos:\n",
    "        \n",
    "        df_nacional, df_estatal = procesar_trimestre_enoe(year, quarter)\n",
    "        \n",
    "        if df_nacional is not None and df_estatal is not None:\n",
    "            resultados_nacionales.append(df_nacional)\n",
    "            resultados_estatales.append(df_estatal)\n",
    "        else:\n",
    "            # Manejo explícito de trimestres sin datos (se añade una fila con NA)\n",
    "            # Esto se asegura de mantener la continuidad de la serie de tiempo.\n",
    "            periodo_na = {'year': year, 'quarter': quarter}\n",
    "            \n",
    "            # Serie Nacional con NA\n",
    "            serie_na_nacional = pd.Series(periodo_na)\n",
    "            # Se añaden las columnas faltantes (variables calculadas) con NaN\n",
    "            if resultados_nacionales:\n",
    "                # Usar la estructura de la primera serie de tiempo para rellenar los NaNs\n",
    "                for col in resultados_nacionales[0].index:\n",
    "                    if col not in serie_na_nacional:\n",
    "                         serie_na_nacional[col] = np.nan\n",
    "            resultados_nacionales.append(serie_na_nacional)\n",
    "            \n",
    "            # DataFrame Estatal con NA\n",
    "            df_na_estatal = pd.DataFrame(periodo_na, index=range(1, 33)) # 32 estados\n",
    "            df_na_estatal['ent_code'] = df_na_estatal.index\n",
    "            df_na_estatal['ent_nombre'] = df_na_estatal['ent_code'].map(ENTIDADES)\n",
    "            # Rellenar todas las columnas de variables con NaN\n",
    "            if resultados_estatales:\n",
    "                 # Usar la estructura del primer DataFrame estatal para rellenar los NaNs\n",
    "                for col in resultados_estatales[0].columns:\n",
    "                    if col not in df_na_estatal.columns:\n",
    "                        df_na_estatal[col] = np.nan\n",
    "            resultados_estatales.append(df_na_estatal)\n",
    "            \n",
    "            print(f\"--- 🚫 Se agregó NA/NaN para {year} T{quarter} y se prosigue. ---\")\n",
    "            \n",
    "\n",
    "    # --- 6. CONSOLIDACIÓN DE BASES DE DATOS ---\n",
    "\n",
    "    # 1. Serie de Tiempo Nacional\n",
    "    df_serie_nacional = pd.DataFrame(resultados_nacionales).reset_index(drop=True)\n",
    "    df_serie_nacional['periodo'] = df_serie_nacional['year'].astype(str) + '-T' + df_serie_nacional['quarter'].astype(str)\n",
    "    df_serie_nacional.set_index('periodo', inplace=True)\n",
    "\n",
    "    print(\"\\n===========================================================\")\n",
    "    print(\"      ✅ BASE DE SERIE DE TIEMPO NACIONAL CREADA\")\n",
    "    print(\"      (Incluye nuevos indicadores estratégicos)\")\n",
    "    print(\"===========================================================\")\n",
    "    print(df_serie_nacional.head())\n",
    "    # Opcional: df_serie_nacional.to_csv(\"serie_tiempo_nacional_estrat.csv\")\n",
    "\n",
    "\n",
    "    # 2. Serie de Tiempo Estatal\n",
    "    df_serie_estatal = pd.concat(resultados_estatales, ignore_index=True)\n",
    "    df_serie_estatal['periodo'] = df_serie_estatal['year'].astype(str) + '-T' + df_serie_estatal['quarter'].astype(str)\n",
    "    \n",
    "    print(\"\\n===========================================================\")\n",
    "    print(\"      ✅ BASE DE SERIE DE TIEMPO ESTATAL CREADA\")\n",
    "    print(\"      (Incluye nuevos indicadores estratégicos)\")\n",
    "    print(\"===========================================================\")\n",
    "    print(df_serie_estatal.head())\n",
    "    # Opcional: df_serie_estatal.to_csv(\"serie_tiempo_estatal_estrat.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
